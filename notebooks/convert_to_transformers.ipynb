{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'has_changed_dir' not in globals():\n",
    "    repo_path = os.path.abspath(os.path.join('..'))\n",
    "    \n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "    \n",
    "    os.chdir(repo_path)\n",
    "    \n",
    "    globals()['has_changed_dir'] = True\n",
    "    print(repo_path)\n",
    "    print(os.getcwd())\n",
    "\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, M2M100Config, NllbTokenizer  # noqa: F401\n",
    "\n",
    "from src.train_model import LightningModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PyTorch Lightning checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model with updated embeddings and tokenizer\n",
    "model_config = \"models/re-init/config.json\"\n",
    "model_path: str = \"models/re-init/\"\n",
    "tokenizer_path: str = \"models/re-init/\"\n",
    "tokenizer_vocab_file_path: str = \"models/re-init/sentencepiece.bpe.model\"\n",
    "\n",
    "# Updated weights to default model with updated embeddings and tokenizer\n",
    "ckpt_path: Optional[Union[str, None]] = \"models/it_1/epoch=17-val_loss=1.48184.ckpt\"\n",
    "\n",
    "# Converted model path\n",
    "converted_model_path = \"models/final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using checkpoint:\n",
    "# Option 1:\n",
    "# model = AutoModelForSeq2SeqLM.from_config(config = M2M100Config.from_json_file(model_config))\n",
    "# Option 2:\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = NllbTokenizer.from_pretrained(tokenizer_path, vocab_file = tokenizer_vocab_file_path)\n",
    "\n",
    "\n",
    "# Option 1:\n",
    "lightining_model = LightningModel.load_from_checkpoint(ckpt_path, map_location=torch.device(\"cuda:1\"), model = model, tokenizer = tokenizer)\n",
    "\n",
    "# Option 2:\n",
    "# ckpt = torch.load(ckpt_path, map_location=torch.device(\"cuda:1\"))\n",
    "# lightining_model = LightningModel(model, tokenizer)\n",
    "# lightining_model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PyTorch Lightning checkpoint to Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    }
   ],
   "source": [
    "# Convert to Transformers\n",
    "lightining_model.convert_ckpt_to_tranformers(converted_model_path)\n",
    "\n",
    "del lightining_model, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(converted_model_path)\n",
    "tokenizer = NllbTokenizer.from_pretrained(converted_model_path, vocab_file = converted_model_path + \"/sentencepiece.bpe.model\")\n",
    "\n",
    "lightining_model = LightningModel(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['И гафарин ван хьайила, Исади абуруз лагьана: - Жерягь сагъбуруз ваъ, начагъбуруз герек я.']\n"
     ]
    }
   ],
   "source": [
    "sentence: str = \"Когда Исо услышал это, Он сказал: – Не здоровым нужен врач, а больным.\"\n",
    "\n",
    "translation = lightining_model.predict(sentence, src_lang='rus_Cyrl' , tgt_lang='lez_Cyrl')\n",
    "\n",
    "print(translation)\n",
    "\n",
    "# Реальный перевод: И гафар ван хьайила, Исади абуруз лагьана: – Жерягь сагъбуруз ваъ, начагъбуруз герек я."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
